{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import standard packages\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import importlib\n",
    "import sklearn\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(528, 12)\n",
      "(462, 12)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "vowel_train = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.train')\n",
    "vowel_test = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.test')\n",
    "print(vowel_train.shape)\n",
    "print(vowel_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (528, 11)\n",
      "y_train shape: (528,)\n",
      "X_test shape: (462, 11)\n",
      "y_test shape: (462,)\n"
     ]
    }
   ],
   "source": [
    "# import sklearn packages\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# divide into X and Y\n",
    "x_train = vowel_train.drop('y', axis=1)\n",
    "y_train = vowel_train['y']\n",
    "x_test = vowel_test.drop('y', axis=1)\n",
    "y_test = vowel_test['y']\n",
    "# convert into numpy array\n",
    "x_train, x_test = np.array(X_train), np.array(X_test)\n",
    "y_train, y_test = np.array(y_train), np.array(y_test)\n",
    "# check results\n",
    "print(\"X_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "n, d = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(X, y, classes, mult):\n",
    "    \"\"\"\n",
    "    Gets appropriate data for ovo or ovr multiclassification.\n",
    "    \"\"\"\n",
    "    if mult == 'ovo':\n",
    "        # get y and indicies\n",
    "        c1, c2 = classes[0], classes[1]\n",
    "        c1_ind, c2_ind = np.where(y == c1)[0], np.where(y == c2)[0]\n",
    "        y_sub = y[(y == c1) | (y == c2)]\n",
    "        # relabel y\n",
    "        y_sub[y_sub == c1] = -1.0\n",
    "        y_sub[y_sub == c2] = 1.0\n",
    "        # get X\n",
    "        X_sub = np.concatenate([X[c1_ind,:], X[c2_ind,:]])\n",
    "        return X_sub, y_sub\n",
    "    elif mult == 'ovr':\n",
    "        # relabel y\n",
    "        y_relab = y.copy()\n",
    "        y_relab[y_relab != classes] = -1.0            \n",
    "        y_relab[y_relab == classes] = 1.0\n",
    "        return X, y_relab\n",
    "\n",
    "x_train, y_train = get_data(x_train, y_train, 1, 'ovr')\n",
    "x_test, y_test = get_data(x_test, y_test, 1, 'ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computegrad(beta, lambduh, x=x_train, y=y_train, h=0.5):\n",
    "    yt = y*x.dot(beta)\n",
    "    ell_prime = -(1+h-yt)/(2*h)*y*(np.abs(1-yt) <= h) - y*(yt < (1-h))\n",
    "    return np.mean(ell_prime[:, np.newaxis]*x, axis=0) + 2*lambduh*beta\n",
    "\n",
    "def objective(beta, lambduh, x=x_train, y=y_train, h=0.5):\n",
    "    yt = y*x.dot(beta)\n",
    "    ell = (1+h-yt)**2/(4*h)*(np.abs(1-yt) <= h) + (1-yt)*(yt < (1-h))\n",
    "    return np.mean(ell) + lambduh*np.dot(beta, beta)\n",
    "\n",
    "def bt_line_search(beta, lambduh, eta=1, alpha=0.5, betaparam=0.8, maxiter=1000, x=x_train, y=y_train):\n",
    "    grad_beta = computegrad(beta, lambduh, x=x, y=y)\n",
    "    norm_grad_beta = np.linalg.norm(grad_beta)\n",
    "    found_eta = 0\n",
    "    iter = 0\n",
    "    while found_eta == 0 and iter < maxiter:\n",
    "        if objective(beta - eta * grad_beta, lambduh, x=x, y=y) < \\\n",
    "            objective(beta, lambduh, x=x, y=y) \\\n",
    "                - alpha * eta * norm_grad_beta ** 2:\n",
    "            found_eta = 1\n",
    "        elif iter == maxiter - 1:\n",
    "            print('Warning: Max number of iterations of' + 'backtracking line search reached')\n",
    "            break\n",
    "        else:\n",
    "            eta *= betaparam\n",
    "            iter += 1\n",
    "    return eta\n",
    "\n",
    "def mylinearsvm(beta_init, theta_init, lambduh, eta_init, maxiter, x=x_train, y=y_train, eps=1e-6):\n",
    "    beta = beta_init\n",
    "    theta = theta_init\n",
    "    grad_theta = computegrad(theta, lambduh, x=x, y=y)\n",
    "    beta_vals = beta\n",
    "    theta_vals = theta\n",
    "    iter = 0\n",
    "    while iter < maxiter and np.linalg.norm(grad_theta) > eps:\n",
    "        eta = bt_line_search(theta, lambduh, eta=eta_init, x=x, y=y)\n",
    "        beta_new = theta - eta*grad_theta\n",
    "        theta = beta_new + iter/(iter+3)*(beta_new-beta)\n",
    "        # Store all of the places we step to\n",
    "        beta_vals = np.vstack((beta_vals, beta_new))\n",
    "        theta_vals = np.vstack((theta_vals, theta))\n",
    "        grad_theta = computegrad(theta, lambduh, x=x, y=y)\n",
    "        beta = beta_new\n",
    "        iter += 1\n",
    "    return beta_vals, theta_vals\n",
    "\n",
    "def misclassification_error(beta_dict, scaler_dict, nclasses, x=x_test, y=y_test):\n",
    "    predictions = np.zeros((len(x), int(nclasses * (nclasses - 1) / 2)))\n",
    "    k = 0\n",
    "    for i in range(1, nclasses + 1):\n",
    "        for j in range(i + 1, nclasses + 1):\n",
    "            # Standardize\n",
    "            xnow = scaler_dict[(i,j)].transform(x)\n",
    "            # Predict\n",
    "            betas = beta_dict[(i, j)]\n",
    "            xbeta = xnow.dot(betas)\n",
    "            predictions[:, k] = (xbeta > 0) * i + (xbeta < 0) * j\n",
    "            k += 1\n",
    "    y_pred = np.ravel(scipy.stats.mode(predictions, axis=1)[0])\n",
    "    assert len(y_pred) == len(y)\n",
    "    return np.mean(y_pred != y), predictions\n",
    "\n",
    "def objective_plot(betas_fg, lambduh, x=x_train, y=y_train, save_file=''):\n",
    "    num_points = np.size(betas_fg, 0)\n",
    "    objs_fg = np.zeros(num_points)\n",
    "    for i in range(0, num_points):\n",
    "        objs_fg[i] = objective(betas_fg[i, :], lambduh, x=x, y=y)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(1, num_points + 1), objs_fg)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Objective value')\n",
    "    plt.title('Objective value vs. iteration when lambda=' + str(lambduh))\n",
    "    if not save_file:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(save_file)\n",
    "\n",
    "def create_train_set(x_train, y_train, i, j):\n",
    "    i_idxs_train = np.where(y_train == i)[0]\n",
    "    xi_train = x_train[i_idxs_train, :]\n",
    "    yi_train = np.ones_like(i_idxs_train)\n",
    "    j_idxs_train = np.where(y_train == j)[0]\n",
    "    xj_train = x_train[j_idxs_train, :]\n",
    "    yj_train = np.ones_like(j_idxs_train) * -1\n",
    "    x_train_c = np.vstack((xi_train, xj_train))\n",
    "    y_train_c = np.concatenate((yi_train, yj_train))\n",
    "    return x_train_c, y_train_c\n",
    "\n",
    "def train(lambduh, x_train, y_train, x_test, y_test):\n",
    "    betas_dict = {}\n",
    "    scaler_dict = {}\n",
    "    nclasses = int(np.max(y_train) - np.min(y_train) + 1)\n",
    "    for i in range(1, nclasses + 1):\n",
    "        for j in range(i + 1, nclasses + 1):\n",
    "            x_train_c, y_train_c = create_train_set(x_train, y_train, i, j)\n",
    "            # Standardize the data.\n",
    "            scaler = sklearn.preprocessing.StandardScaler()\n",
    "            x_train_c = scaler.fit_transform(x_train_c)\n",
    "            scaler_dict[(i,j)] = scaler\n",
    "            # Initialize\n",
    "            beta_init = np.zeros(d)\n",
    "            theta_init = np.zeros(d)\n",
    "            eta_init = 1 / (scipy.linalg.eigh(1 / len(y_train_c) * \\\n",
    "            x_train_c.T.dot(x_train_c), eigvals=(d - 1, d - 1), eigvals_only=True)[0] + lambduh)\n",
    "            maxiter = 100\n",
    "            # Fit the model\n",
    "            betas_fastgrad, _ = mylinearsvm(beta_init, theta_init,\n",
    "            lambduh, eta_init, maxiter, x=x_train_c, y=y_train_c)\n",
    "            betas_dict[(i, j)] = betas_fastgrad[-1, :]\n",
    "    test_error, preds = misclassification_error(betas_dict, scaler_dict, nclasses, x_test, y_test)\n",
    "    return test_error, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-1f8060f8d4db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlambduh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambduh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test misclassification error when lambda='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambduh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-13e30e2edd63>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(lambduh, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m    105\u001b[0m             betas_fastgrad, _ = mylinearsvm(beta_init, theta_init,\n\u001b[1;32m    106\u001b[0m             lambduh, eta_init, maxiter, x=x_train_c, y=y_train_c)\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mbetas_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbetas_fastgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mtest_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmisclassification_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetas_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "lambduh = 1.0\n",
    "test_error, preds_corr = train(lambduh, x_train, y_train, x_test, y_test)\n",
    "print('Test misclassification error when lambda=', lambduh, ':', test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  3.  4.  1.  6.  7.  1.  1.  1. 11.  3.  2.  2.  2.  2.  2.  2.  2.\n",
      "  2.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4. 11.  6.  7.\n",
      "  5.  9.  5. 11.  6.  6.  6.  6. 11.  7.  7.  7. 11.  9.  8. 11.  9. 11.\n",
      " 11.]\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "i = 50\n",
    "print(preds_corr[i,:])\n",
    "print(scipy.stats.mode(preds_corr[i,:])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(X, y, lambda_max=2 ** 3, max_iter=1000, num_lambda=10, num_folds=5, seed=0):\n",
    "    # Run cross-validation to find the optimal lambda\n",
    "    # Randomly divide the data into num_folds parts\n",
    "    np.random.seed(seed)\n",
    "    n = len(y)\n",
    "    order = list(range(n))\n",
    "    np.random.shuffle(order)\n",
    "    idxs = [order[int(i * n / num_folds):int((i + 1) * n / num_folds)] for i in range(num_folds)]\n",
    "    lambduh = lambda_max\n",
    "    beta = np.zeros(np.size(X, 1))\n",
    "    best_error = np.inf\n",
    "    print('lambda \\t Misclassification error')\n",
    "    # Try many possible lambdas and see which gives the lowest\n",
    "    # average mse on the test set\n",
    "    for j in range(num_lambda):\n",
    "        avg_error = 0\n",
    "        for part_num in range(num_folds):\n",
    "            # Create the training and test sets\n",
    "            x_test = X[idxs[part_num], :]\n",
    "            y_test = y[idxs[part_num]]\n",
    "            train_idxs = idxs[0:part_num] + idxs[part_num + 1:]\n",
    "            train_idxs = [item for sublist in train_idxs for item in sublist]\n",
    "            x_train = X[train_idxs, :]\n",
    "            y_train = y[train_idxs]\n",
    "            # Compute the optimal betas. Use warm-start.\n",
    "            error = train(lambduh, x_train, y_train, x_test, y_test)\n",
    "            avg_error += error\n",
    "        avg_error /= num_folds\n",
    "        print('%0.2e' % lambduh, '\\t', np.round(avg_error, 5))\n",
    "        # Update the best error\n",
    "        if avg_error < best_error:\n",
    "            best_error = avg_error\n",
    "            best_lambda = lambduh\n",
    "        lambduh /= 2\n",
    "    return best_lambda, best_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda \t Misclassification error\n",
      "8.00e+00 \t 0.41279\n",
      "4.00e+00 \t 0.41279\n",
      "2.00e+00 \t 0.40331\n",
      "1.00e+00 \t 0.35786\n",
      "5.00e-01 \t 0.28591\n",
      "2.50e-01 \t 0.24047\n",
      "1.25e-01 \t 0.21588\n",
      "6.25e-02 \t 0.20836\n",
      "3.12e-02 \t 0.18753\n",
      "1.56e-02 \t 0.16293\n",
      "7.81e-03 \t 0.15344\n",
      "3.91e-03 \t 0.14593\n",
      "1.95e-03 \t 0.14397\n",
      "9.77e-04 \t 0.14774\n",
      "4.88e-04 \t 0.14963\n",
      "2.44e-04 \t 0.15152\n",
      "1.22e-04 \t 0.14961\n",
      "6.10e-05 \t 0.1496\n",
      "3.05e-05 \t 0.1496\n",
      "1.53e-05 \t 0.1496\n",
      "7.63e-06 \t 0.1496\n",
      "3.81e-06 \t 0.1496\n",
      "1.91e-06 \t 0.1496\n",
      "9.54e-07 \t 0.1496\n",
      "4.77e-07 \t 0.1496\n",
      "The best lambda found was lambda = 1.95e-03\n",
      "Test misclassification error when lambda= 0.001953125 : 0.525974025974026\n"
     ]
    }
   ],
   "source": [
    "best_lambda, error = cross_validate(x_train, y_train, num_lambda=25)\n",
    "print('The best lambda found was lambda = %0.2e' % best_lambda)\n",
    "test_error = train(best_lambda, x_train, y_train, x_test, y_test)\n",
    "print('Test misclassification error when lambda=', best_lambda, ':', test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
